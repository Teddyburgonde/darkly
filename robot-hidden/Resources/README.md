aller sur robots.txt

il y a deux chemins disallow :
.hidden

on a crée un script python pour parcourir tous les fichiers et trouver le flag.



Comment l'éviter ?
- Dans le robots.txt on peut disallow tout et allow seulement les pages accessibles à un utilisateur lambda.
- Désactiver l'indexation des fichiers du repertoire en configurant correctement le server web.

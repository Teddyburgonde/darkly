/robots.txt

## Comment ?

Il y a deux chemins "disallow" :
.hidden
whatever

Ici on utilise .hidden  
On a créé un script python pour parcourir tous les fichiers et trouver le flag.


## Comment l'éviter ?
- Dans le robots.txt on peut disallow tout et allow seulement les pages accessibles à un utilisateur lambda.
- Désactiver l'indexation des fichiers du repertoire en configurant correctement le server web.
